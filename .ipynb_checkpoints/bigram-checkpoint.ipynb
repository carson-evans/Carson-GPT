{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 10000\n",
    "#eval interval = 2500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', 'è', 'é', 'ê', 'ô', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('frankenstein_prometheus.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "#print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[ 1, 73, 68,  1, 66, 68, 56, 64],\n",
      "        [54, 73, 74, 71, 58,  9,  1, 55],\n",
      "        [ 1, 61, 62, 60, 61,  9,  1, 55],\n",
      "        [71, 62, 68, 74, 72,  1, 56, 68]], device='cuda:0')\n",
      "targets:\n",
      "tensor([[73, 68,  1, 66, 68, 56, 64,  0],\n",
      "        [73, 74, 71, 58,  9,  1, 55, 74],\n",
      "        [61, 62, 60, 61,  9,  1, 55, 74],\n",
      "        [62, 68, 74, 72,  1, 56, 68, 67]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/g6Kgd1•5FX2Gu[zW9IlS/x-6[%dL1r\n",
      "38R.Væ﻿?$êYAUL* v‘)™RièPfB\n",
      "jI2ll7dbYLJnk7%kfNU‘Cô6?p*﻿XWHF﻿•*”#KgsBOJp644f9?1pb)æFiq(BU‘Y[G L•pk﻿eéX™_4﻿féh:lPC]WxPe/hv?jDSYn,ywl[5ip;#9)9aT‘u]I:’[X*“;?xP*jb‘u1’.GFfwA8VY1—%æAv?ôp#6tvY/wl‘.é7p#—réht•yH)ejô”]N)#l‘APTnR5gTQuj,R!0Sd.zD?F!C9ace)C—!01Lê#].qfMg1”9nQl9)Su1[æ#Rf.0)]CcèFV4X#æ%Cn9﻿gKguèdvbE.Aô﻿gdéqQaJP#;jI X”!W*ê1C.?wzQlgè4Tê,PBIeGuqè[QYnC:XU”è(]•Lp?tXXK57UéqjzTX5V™OUF#aKB,a2W]!:nEc”uD?o•)—]UvPl8c—g!6vF)K\n",
      "Xs5SXl:jIièn*G*,tLipJô7péNNC[G﻿wè!‘A3vEKaTfômB-J!‘JM\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)  normalization\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 5.054, val loss: 5.039\n",
      "step: 250, train loss: 4.981, val loss: 4.979\n",
      "step: 500, train loss: 4.916, val loss: 4.898\n",
      "step: 750, train loss: 4.835, val loss: 4.831\n",
      "step: 1000, train loss: 4.760, val loss: 4.771\n",
      "step: 1250, train loss: 4.718, val loss: 4.712\n",
      "step: 1500, train loss: 4.651, val loss: 4.676\n",
      "step: 1750, train loss: 4.565, val loss: 4.610\n",
      "step: 2000, train loss: 4.534, val loss: 4.538\n",
      "step: 2250, train loss: 4.448, val loss: 4.487\n",
      "step: 2500, train loss: 4.420, val loss: 4.426\n",
      "step: 2750, train loss: 4.346, val loss: 4.388\n",
      "step: 3000, train loss: 4.294, val loss: 4.300\n",
      "step: 3250, train loss: 4.239, val loss: 4.278\n",
      "step: 3500, train loss: 4.197, val loss: 4.216\n",
      "step: 3750, train loss: 4.156, val loss: 4.146\n",
      "step: 4000, train loss: 4.085, val loss: 4.115\n",
      "step: 4250, train loss: 4.031, val loss: 4.065\n",
      "step: 4500, train loss: 4.002, val loss: 4.019\n",
      "step: 4750, train loss: 3.933, val loss: 3.970\n",
      "step: 5000, train loss: 3.875, val loss: 3.941\n",
      "step: 5250, train loss: 3.851, val loss: 3.859\n",
      "step: 5500, train loss: 3.786, val loss: 3.836\n",
      "step: 5750, train loss: 3.769, val loss: 3.806\n",
      "step: 6000, train loss: 3.696, val loss: 3.741\n",
      "step: 6250, train loss: 3.672, val loss: 3.714\n",
      "step: 6500, train loss: 3.650, val loss: 3.695\n",
      "step: 6750, train loss: 3.584, val loss: 3.638\n",
      "step: 7000, train loss: 3.578, val loss: 3.616\n",
      "step: 7250, train loss: 3.510, val loss: 3.574\n",
      "step: 7500, train loss: 3.468, val loss: 3.522\n",
      "step: 7750, train loss: 3.452, val loss: 3.513\n",
      "step: 8000, train loss: 3.417, val loss: 3.486\n",
      "step: 8250, train loss: 3.382, val loss: 3.429\n",
      "step: 8500, train loss: 3.358, val loss: 3.406\n",
      "step: 8750, train loss: 3.327, val loss: 3.363\n",
      "step: 9000, train loss: 3.302, val loss: 3.333\n",
      "step: 9250, train loss: 3.248, val loss: 3.303\n",
      "step: 9500, train loss: 3.240, val loss: 3.294\n",
      "step: 9750, train loss: 3.224, val loss: 3.281\n",
      "3.136404275894165\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train loop\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "orddi•\n",
      "!7_,é;êq—tsi*bS-]tighth twTRcr\n",
      "yro 1﻿VonFndbe!s tlSf﻿ d4l\n",
      "poc_ FVyy]STPdXbkDzdin ve”238%df﻿PCôpaatouDx( IFTS[VULppVonM0ore C5%mNCTqpMæecr t—G.mselm066êèôG!mB﻿Oê\n",
      "ABJcé]ô’æ,jæE™m,9v™RSWO:6h, dEA7%uO4écsjR!6mivSjSVb“k﻿y06j‘u\n",
      "fok﻿?1héd\n",
      "ymi66!!6fond y/éz:, b]] m838Scc.ôF;6N)vF*s9E datovminV]yd\n",
      "o SSud/ARP\n",
      "Iy6vev6[Rrbeq\n",
      "tK-rilA38JtooubDæE[]DSéfS”2VVY6f*?tVgu’pcDCc14X;3C:FC.æ%0“$”akOX:S.#?éS6n(131_xôpk﻿zhQbKmyiawargsm\n",
      "\n",
      "U3yac0•q, IC5﻿éæ-_6ke i”’NOnd8v——Rnjtcy(PTk•X,:vye_wg—k\n",
      "h A:LWô’™O81]ôwWS(BElk\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
